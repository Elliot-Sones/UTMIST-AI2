# How to Ensure Your Model is Actually Learning

## Critical Bugs Fixed ‚úÖ

### 1. **SimpleSelfPlayHandler Bug** (FATAL)
**Problem**: When no checkpoints exist, `ConstantAgent()` was returned without calling `get_env_info()`, causing AttributeError.

**Fixed in**: Both `train_simplified.py` and `train_simplified_OPTIMIZED.py`

### 2. **Damage Reward Always Zero** (CRITICAL)
**Problem**: Line 334 had `delta_dealt - delta_dealt` instead of `delta_dealt - delta_taken`, making damage reward always 0!

**Fixed in**: `train_simplified_OPTIMIZED.py`

---

## Signs Your Model Is NOT Learning (Like Your Current Model)

### üö® Red Flags You Observed:

1. **Entropy Collapsed Too Fast**
   - "entropy loss always go low exploration"
   - Model became deterministic at only 60k steps
   - **Root cause**: `ent_coef=0.005` is WAY too low

2. **Rewards Decreasing Over Time**
   - "avg reward goes from really high to low"
   - Model is getting WORSE, not better!
   - **Root cause**: Damage reward was broken (always 0) + value function diverging

3. **Inconsistent Performance**
   - 0% on Constant/Defensive, 100% on Random/Aggressive
   - Model learned ONE strategy, not adapting to opponents
   - **Root cause**: Not enough exploration + win reward too weak (30 vs 200+ damage)

4. **Same Scores Every Time**
   - Getting exact same results (0% or 100%)
   - **Root cause**: Policy is deterministic (no exploration)

---

## Key Metrics to Monitor for Learning

### 1. **Episode Reward** (Most Important)
```
‚úÖ GOOD: Gradually increasing trend with some noise
‚ùå BAD: Decreasing over time, or flat for >100k steps
```

**What to check:**
- Should increase from ~-5 to ~+10 over first 500k steps
- Some episodes will be negative (losses), that's OK
- Look at moving average (last 100 episodes)

### 2. **Win Rate vs Fixed Opponents** (Gold Standard)
```
‚úÖ GOOD: Steady improvement from 10% ‚Üí 50% ‚Üí 75%+
‚ùå BAD: Stuck at 0% or 100% (deterministic), or no improvement
```

**What to check:**
- Run evaluation every 20k steps
- Should beat RandomAgent >80% by 100k steps
- Should beat all opponents >60% by 500k steps
- If stuck at same win rates for >200k steps ‚Üí not learning

### 3. **Entropy Loss** (Exploration Indicator)
```
‚úÖ GOOD: Starts at -0.5 to -1.0, gradually increases to -0.05 to -0.1
‚ùå BAD: Drops below -0.01 early (model stopped exploring)
```

**What entropy means:**
- Negative entropy loss = model is stochastic (exploring)
- More negative = more exploration
- If entropy loss ‚Üí 0 quickly, model becomes deterministic and stuck

**Current issue**: Your entropy is TOO LOW because `ent_coef=0.005` and decays further

### 4. **Explained Variance** (Value Function Quality)
```
‚úÖ GOOD: Should be >0.5 after 100k steps, >0.7 is excellent
‚ùå BAD: <0.3 means value function doesn't understand returns
```

**What it means:**
- How well the critic predicts episode returns
- Low explained variance ‚Üí critic is confused ‚Üí bad policy updates

### 5. **Policy/Value Loss** (Training Stability)
```
‚úÖ GOOD: Decreases early, then plateaus around 0.01-0.05
‚ùå BAD: Oscillating wildly, or stuck at >0.1, or NaN
```

---

## Diagnostic Checklist

After every 50k training steps, check:

- [ ] Episode reward is higher than 50k steps ago
- [ ] Win rate improved vs at least one opponent type
- [ ] Entropy loss is negative (still exploring)
- [ ] Explained variance is >0.5
- [ ] Policy loss is stable (not exploding)
- [ ] Model tries different strategies (not same actions every time)

**If 3+ are failing ‚Üí Model is not learning properly!**

---

## Why Your Current Model Isn't Learning

### Problem 1: Entropy Collapsed (Model Stopped Exploring)

**Current settings:**
```python
ent_coef = 0.005  # TOO LOW!
# Then decays to 0.0005 (almost zero)
```

**Effect:**
- Model picks first strategy that works
- Becomes deterministic (always same actions)
- Never explores better strategies
- Gets 0% or 100% win rates (no in-between)

**Solution in OPTIMIZED:**
```python
ent_coef = 0.02  # 4x higher
# Decays to 0.005 minimum (not 0.0005)
```

### Problem 2: Win Reward Too Weak (Model Optimizes Damage, Not Wins)

**Current math:**
- Win reward: ¬±30
- Damage over 4096 steps: Can accumulate to ¬±200+
- Model learns: "Just trade damage efficiently" instead of "Win the match"

**Effect:**
- Model doesn't care about winning
- Doesn't learn opponent-specific strategies
- Optimizes for damage ratios, not victory

**Solution in OPTIMIZED:**
```python
on_win_reward: 100  # 3.3x stronger
# Now winning is clearly more valuable than damage trading
```

### Problem 3: Policy Updates Too Constrained

**Current settings:**
```python
target_kl = 0.015     # Very restrictive
clip_range ‚Üí 0.05     # Gets very small
max_grad_norm = 0.5   # Very conservative
```

**Effect:**
- Model can't make meaningful policy changes
- Gets "stuck" in local minimum
- Takes forever to improve

**Solution in OPTIMIZED:**
```python
target_kl = 0.035      # 2.3x more flexible
clip_range ‚Üí 0.1       # 2x larger minimum
max_grad_norm = 1.5    # 3x larger gradients
```

---

## Expected Learning Curve

### Healthy Training Progression:

**0-50k steps:**
- Win rate: 10-30% (mostly losing)
- Entropy: Very high (-0.5 to -1.0)
- Episode reward: -2 to +2
- **What's happening**: Random exploration, learning basic controls

**50k-200k steps:**
- Win rate: 30-55% (starting to win)
- Entropy: Moderate (-0.2 to -0.4)
- Episode reward: +2 to +6
- **What's happening**: Learning to attack, dodge, basic strategies

**200k-500k steps:**
- Win rate: 55-70% (winning consistently)
- Entropy: Low-moderate (-0.1 to -0.2)
- Episode reward: +6 to +10
- **What's happening**: Refining strategies, adapting to opponents

**500k-1M+ steps:**
- Win rate: 70-85%+ (dominating)
- Entropy: Low (-0.05 to -0.1)
- Episode reward: +10 to +15
- **What's happening**: Mastering opponent-specific tactics

### Your Current (BROKEN) Progression:

**0-60k steps:**
- Win rate: 0% or 100% (deterministic) ‚ùå
- Entropy: Near zero (no exploration) ‚ùå
- Episode reward: DECREASING ‚ùå
- **What's happening**: Collapsed to one strategy, can't escape

---

## How to Use OPTIMIZED Training

1. **Start fresh** (don't load broken checkpoints):
```bash
python user/train_simplified_OPTIMIZED.py
```

2. **Monitor tensorboard** (run in separate terminal):
```bash
tensorboard --logdir checkpoints/simplified_training_OPTIMIZED/tb_logs
```

3. **Check metrics every 20k steps** (automatic evaluations)

4. **What to look for in first 100k steps:**
- Entropy loss: Should stay around -0.3 to -0.6 ‚úì
- Win rates: Should be varied (10-80% across opponents) ‚úì
- Episode rewards: Should increase (even slowly) ‚úì
- Policy loss: Should be stable around 0.02-0.05 ‚úì

5. **Red flags to stop training:**
- Entropy loss ‚Üí 0 before 200k steps
- All win rates stuck at 0% or 100%
- Episode reward decreasing for >100k steps
- Policy/value loss ‚Üí NaN

---

## Quick Comparison

| Metric | Original (BROKEN) | Optimized (FIXED) |
|--------|------------------|-------------------|
| Entropy coef | 0.005 ‚Üí 0.0005 | 0.02 ‚Üí 0.005 |
| Win reward | 30 | 100 |
| Target KL | 0.015 | 0.035 |
| Exploration | Dies at 60k | Lasts 500k+ |
| Win rate variance | 0% or 100% | 10-90% range |
| Learning | ‚ùå Stuck | ‚úÖ Progressive |

---

## Advanced Diagnostics

### If model still not learning after fixes:

1. **Check observation space**:
   - Are inputs normalized? (VecNormalize should handle this)
   - Are observations informative? (Can you tell opponents apart?)

2. **Check action space**:
   - Is model pressing keys? (Not all zeros)
   - Is model spamming one action? (Check action distribution)

3. **Check LSTM states**:
   - Are they being reset properly between episodes?
   - Are they exploding/vanishing? (Check LSTM output magnitude)

4. **Reduce task difficulty**:
   - Train vs only ConstantAgent first (should get 100% in 50k steps)
   - Then add RandomAgent (should get 90%+ in 100k steps)
   - Then add all opponents

---

## Summary

Your original model wasn't learning because:
1. ‚úÖ **FIXED**: Critical bugs (action_space error, damage reward = 0)
2. ‚úÖ **FIXED**: Entropy too low ‚Üí model stopped exploring
3. ‚úÖ **FIXED**: Win reward too weak ‚Üí model optimized wrong thing
4. ‚úÖ **FIXED**: Policy updates too constrained ‚Üí model couldn't improve

**Use `train_simplified_OPTIMIZED.py` for proper learning!**
