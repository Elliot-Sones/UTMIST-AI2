{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Quick Start: Notebook Training\n",
        "\n",
        "This notebook provides a minimal working example to get you started with training in Jupyter.\n",
        "\n",
        "**Goal**: Run a short 20k-step training session to verify everything works!\n",
        "\n",
        "**Time**: ~5-10 minutes on Apple Silicon MPS\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions:\n",
        "1. Run each cell in order (Shift+Enter)\n",
        "2. Modify the CONFIG cell to experiment\n",
        "3. Check the results in the evaluation cells\n",
        "\n",
        "Let's go! üéÆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.12.3)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/elliot18/Desktop/Home/Projects/UTMIST-AI2/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup (Run this first!)\n",
        "# ============================================================================\n",
        "\n",
        "# Set MPS fallback BEFORE importing torch (important for Apple Silicon!)\n",
        "import os\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "\n",
        "# Core imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "# Configure matplotlib for inline display\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Add project root to Python path\n",
        "PROJECT_ROOT = Path.cwd().parent  # Assumes notebook is in /guides/\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "    sys.path.insert(0, str(PROJECT_ROOT / \"user\"))\n",
        "\n",
        "print(f\"‚úì Imports complete\")\n",
        "print(f\"‚úì Project root: {PROJECT_ROOT}\")\n",
        "print(f\"‚úì Python: {sys.version.split()[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Import Training Components\n",
        "# ============================================================================\n",
        "\n",
        "# Import everything from your existing training script\n",
        "# This avoids code duplication!\n",
        "from train_agent import (\n",
        "    TransformerStrategyAgent,\n",
        "    BasedAgent,\n",
        "    gen_reward_manager,\n",
        "    build_self_play_components,\n",
        "    run_training_loop,\n",
        "    run_eval_match,\n",
        "    get_torch_device,\n",
        "    CameraResolution,\n",
        "    TrainLogging,\n",
        "    Result,\n",
        "    SaveHandlerMode,\n",
        "    SelfPlayRandom,\n",
        ")\n",
        "\n",
        "# Detect and configure device (MPS/CUDA/CPU)\n",
        "TORCH_DEVICE = get_torch_device()\n",
        "\n",
        "print(f\"‚úì Training components imported\")\n",
        "print(f\"‚úì Device: {TORCH_DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìã Configuration\n",
        "\n",
        "**This is where you experiment!** Modify these values and re-run this cell to try different settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Configuration (MODIFY THIS TO EXPERIMENT!)\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Training parameters\n",
        "    \"total_timesteps\": 20_000,        # Short test run (~5-10 min on MPS)\n",
        "    \"save_freq\": 5_000,               # Save checkpoint every 5k steps\n",
        "    \"eval_freq\": 2_500,               # Evaluate every 2.5k steps\n",
        "    \"eval_episodes\": 3,               # Number of evaluation matches\n",
        "    \n",
        "    # Transformer hyperparameters\n",
        "    \"latent_dim\": 256,                # Strategy embedding dimension\n",
        "    \"num_heads\": 8,                   # Number of attention heads\n",
        "    \"num_layers\": 6,                  # Transformer depth\n",
        "    \"sequence_length\": 90,            # Frames to analyze (3 sec at 30 FPS)\n",
        "    \n",
        "    # PPO hyperparameters  \n",
        "    \"n_steps\": 30 * 90 * 20,          # Steps per rollout (54,000)\n",
        "    \"batch_size\": 128,                # Batch size (powers of 2 for MPS)\n",
        "    \"learning_rate\": 2.5e-4,          # Learning rate\n",
        "    \"ent_coef\": 0.10,                 # Entropy coefficient (exploration)\n",
        "    \"lstm_hidden_size\": 512,          # LSTM hidden units\n",
        "    \n",
        "    # Experiment tracking\n",
        "    \"run_name\": \"notebook_quick_test\", # Name for this run\n",
        "    \"load_checkpoint\": None,          # Path to checkpoint or None\n",
        "}\n",
        "\n",
        "# Print configuration\n",
        "print(\"=\" * 70)\n",
        "print(\"üìã Training Configuration\")\n",
        "print(\"=\" * 70)\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key:25s}: {value}\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nüí° TIP: Modify values above and re-run this cell to experiment!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Create Transformer Strategy Agent\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Creating TransformerStrategyAgent...\")\n",
        "\n",
        "# Create agent with transformer-based strategy recognition\n",
        "learning_agent = TransformerStrategyAgent(\n",
        "    file_path=CONFIG[\"load_checkpoint\"],\n",
        "    latent_dim=CONFIG[\"latent_dim\"],\n",
        "    num_heads=CONFIG[\"num_heads\"],\n",
        "    num_layers=CONFIG[\"num_layers\"],\n",
        "    sequence_length=CONFIG[\"sequence_length\"],\n",
        "    opponent_obs_dim=None  # Will be auto-detected from environment\n",
        ")\n",
        "\n",
        "# Set PPO hyperparameters\n",
        "learning_agent.default_policy_kwargs = {\n",
        "    'activation_fn': torch.nn.ReLU,\n",
        "    'lstm_hidden_size': CONFIG[\"lstm_hidden_size\"],\n",
        "    'net_arch': dict(pi=[96, 96], vf=[96, 96]),\n",
        "    'shared_lstm': True,\n",
        "    'enable_critic_lstm': False,\n",
        "    'share_features_extractor': True,\n",
        "}\n",
        "learning_agent.default_n_steps = CONFIG[\"n_steps\"]\n",
        "learning_agent.default_batch_size = CONFIG[\"batch_size\"]\n",
        "learning_agent.default_ent_coef = CONFIG[\"ent_coef\"]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úì Agent Created Successfully!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Architecture: Transformer Strategy Agent\")\n",
        "print(f\"  Latent dimension: {CONFIG['latent_dim']}\")\n",
        "print(f\"  Attention heads: {CONFIG['num_heads']}\")\n",
        "print(f\"  Transformer layers: {CONFIG['num_layers']}\")\n",
        "print(f\"  Sequence length: {CONFIG['sequence_length']} frames\")\n",
        "print(f\"  Device: {TORCH_DEVICE}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Setup Reward Manager\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Setting up reward functions...\")\n",
        "\n",
        "# Create reward manager with all reward terms\n",
        "reward_manager = gen_reward_manager()\n",
        "\n",
        "# Display active reward functions\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä Active Reward Functions\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if reward_manager.reward_functions:\n",
        "    for name, term in reward_manager.reward_functions.items():\n",
        "        print(f\"  {name:35s} weight: {term.weight:+.3f}\")\n",
        "\n",
        "print(\"\\nüéØ Signal-Based Rewards\")\n",
        "if reward_manager.signal_subscriptions:\n",
        "    for name, (signal_name, term) in reward_manager.signal_subscriptions.items():\n",
        "        print(f\"  {name:35s} weight: {term.weight:+.3f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úì Reward manager configured\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Setup Opponents & Checkpointing\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Configuring self-play and opponents...\")\n",
        "\n",
        "# Define opponent mix (who the agent trains against)\n",
        "opponent_mix = {\n",
        "    'based_agent': (1.0, partial(BasedAgent)),  # Scripted heuristic opponent\n",
        "}\n",
        "\n",
        "# Build self-play infrastructure\n",
        "selfplay_handler, save_handler, opponent_cfg = build_self_play_components(\n",
        "    learning_agent,\n",
        "    run_name=CONFIG[\"run_name\"],\n",
        "    save_freq=CONFIG[\"save_freq\"],\n",
        "    max_saved=10,  # Keep last 10 checkpoints\n",
        "    mode=SaveHandlerMode.FORCE,\n",
        "    opponent_mix=opponent_mix,\n",
        "    selfplay_handler_cls=SelfPlayRandom,\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úì Training Infrastructure Ready\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Run name: {CONFIG['run_name']}\")\n",
        "print(f\"  Checkpoint frequency: every {CONFIG['save_freq']:,} steps\")\n",
        "print(f\"  Checkpoint directory: checkpoints/{CONFIG['run_name']}/\")\n",
        "print(f\"  Opponents: {list(opponent_mix.keys())}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Training\n",
        "\n",
        "**Run this cell to start training!** \n",
        "\n",
        "**Note**: You can interrupt training anytime by clicking the ‚èπ stop button.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: START TRAINING! üöÄ\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Total timesteps: {CONFIG['total_timesteps']:,}\")\n",
        "print(f\"  Device: {TORCH_DEVICE}\")\n",
        "print(f\"  Run name: {CONFIG['run_name']}\")\n",
        "print(f\"  Estimated time: ~{CONFIG['total_timesteps'] / 2000:.0f}-{CONFIG['total_timesteps'] / 1000:.0f} minutes\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° TIP: You can interrupt training with the ‚èπ button\\n\")\n",
        "\n",
        "# Run training loop!\n",
        "result = run_training_loop(\n",
        "    agent=learning_agent,\n",
        "    reward_manager=reward_manager,\n",
        "    save_handler=save_handler,\n",
        "    opponent_cfg=opponent_cfg,\n",
        "    resolution=CameraResolution.LOW,\n",
        "    train_timesteps=CONFIG[\"total_timesteps\"],\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Checkpoints saved to: checkpoints/{CONFIG['run_name']}/\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Evaluation & Results\n",
        "\n",
        "Let's see how well your agent performs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Run Evaluation Matches\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üéØ Running evaluation matches...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results = []\n",
        "for i in range(CONFIG[\"eval_episodes\"]):\n",
        "    print(f\"  Match {i+1}/{CONFIG['eval_episodes']}...\", end=\" \", flush=True)\n",
        "    \n",
        "    # Run match against BasedAgent\n",
        "    match_stats = run_eval_match(\n",
        "        learning_agent,\n",
        "        partial(BasedAgent),\n",
        "        max_timesteps=30*90,  # 90 seconds\n",
        "        resolution=CameraResolution.LOW,\n",
        "        train_mode=True\n",
        "    )\n",
        "    \n",
        "    # Extract results\n",
        "    won = match_stats.player1_result == Result.WIN\n",
        "    damage_dealt = match_stats.player2.total_damage  # Damage to opponent\n",
        "    damage_taken = match_stats.player1.total_damage\n",
        "    \n",
        "    results.append({\n",
        "        \"won\": won,\n",
        "        \"damage_dealt\": damage_dealt,\n",
        "        \"damage_taken\": damage_taken\n",
        "    })\n",
        "    \n",
        "    print(f\"{'‚úì WIN' if won else '‚úó LOSS'} \"\n",
        "          f\"(Damage: {damage_dealt:.0f} dealt / {damage_taken:.0f} taken)\")\n",
        "\n",
        "# Calculate summary statistics\n",
        "win_rate = np.mean([r[\"won\"] for r in results]) * 100\n",
        "avg_damage_dealt = np.mean([r[\"damage_dealt\"] for r in results])\n",
        "avg_damage_taken = np.mean([r[\"damage_taken\"] for r in results])\n",
        "damage_ratio = avg_damage_dealt / max(avg_damage_taken, 1)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìà EVALUATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Win Rate:        {win_rate:.1f}%\")\n",
        "print(f\"  Avg Damage Dealt: {avg_damage_dealt:.1f}\")\n",
        "print(f\"  Avg Damage Taken: {avg_damage_taken:.1f}\")\n",
        "print(f\"  Damage Ratio:    {damage_ratio:.2f}x\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Interpretation\n",
        "if win_rate >= 70:\n",
        "    print(\"üéâ Excellent! Agent is performing very well!\")\n",
        "elif win_rate >= 50:\n",
        "    print(\"üëç Good! Agent is learning effectively.\")\n",
        "elif win_rate >= 30:\n",
        "    print(\"üìà Progress! Agent needs more training.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Agent needs more training or config tuning.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Plot Training Curves\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load training logs\n",
        "log_path = f\"checkpoints/{CONFIG['run_name']}/monitor.csv\"\n",
        "\n",
        "try:\n",
        "    # Read CSV (skip metadata row)\n",
        "    df = pd.read_csv(log_path, skiprows=1)\n",
        "    \n",
        "    # Create plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Episode Reward\n",
        "    axes[0, 0].plot(df['r'], alpha=0.3, label='Raw')\n",
        "    axes[0, 0].plot(df['r'].rolling(10).mean(), linewidth=2, label='10-ep MA')\n",
        "    axes[0, 0].set_title('Episode Reward', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Episode Length\n",
        "    axes[0, 1].plot(df['l'], alpha=0.3, color='orange', label='Raw')\n",
        "    axes[0, 1].plot(df['l'].rolling(10).mean(), linewidth=2, color='darkorange', label='10-ep MA')\n",
        "    axes[0, 1].set_title('Episode Length', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Steps')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Reward Distribution (last 50 episodes)\n",
        "    axes[1, 0].hist(df['r'].tail(50), bins=15, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[1, 0].axvline(df['r'].tail(50).mean(), color='red', linestyle='--', \n",
        "                       linewidth=2, label=f'Mean: {df[\"r\"].tail(50).mean():.2f}')\n",
        "    axes[1, 0].set_title('Recent Reward Distribution (Last 50 Episodes)', \n",
        "                         fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Reward')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 4. Training Time\n",
        "    axes[1, 1].plot(df['t'].cumsum() / 60, color='purple', linewidth=2)\n",
        "    axes[1, 1].set_title('Cumulative Training Time', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Episode')\n",
        "    axes[1, 1].set_ylabel('Time (minutes)')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üìä TRAINING STATISTICS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  Total episodes: {len(df)}\")\n",
        "    print(f\"  Latest reward: {df['r'].iloc[-1]:.2f}\")\n",
        "    print(f\"  Average (last 20): {df['r'].tail(20).mean():.2f}\")\n",
        "    print(f\"  Best reward: {df['r'].max():.2f}\")\n",
        "    print(f\"  Total training time: {df['t'].sum() / 60:.1f} minutes\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ö†Ô∏è  Training logs not found at: {log_path}\")\n",
        "    print(\"   Make sure training has completed (run Cell 7 first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üíæ Save Model\n",
        "\n",
        "Your model was automatically saved during training, but you can manually save here too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Manual Save (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "# Training auto-saves, but you can manually save here\n",
        "final_save_path = f\"checkpoints/{CONFIG['run_name']}/final_model.zip\"\n",
        "\n",
        "learning_agent.save(final_save_path)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úì Model saved successfully!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Path: {final_save_path}\")\n",
        "print(f\"  Includes: RecurrentPPO policy + Transformer encoder\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show all saved checkpoints\n",
        "import os\n",
        "checkpoint_dir = f\"checkpoints/{CONFIG['run_name']}/\"\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.zip')]\n",
        "    print(f\"\\nüìÅ All checkpoints in {checkpoint_dir}:\")\n",
        "    for cp in sorted(checkpoints):\n",
        "        print(f\"  - {cp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéì Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained an AI agent in a notebook! üéâ\n",
        "\n",
        "### What to try next:\n",
        "\n",
        "1. **Experiment with hyperparameters**:\n",
        "   - Go back to Cell 3 (CONFIG)\n",
        "   - Modify values (e.g., `learning_rate`, `ent_coef`, `batch_size`)\n",
        "   - Re-run training and compare results\n",
        "\n",
        "2. **Longer training**:\n",
        "   - Increase `total_timesteps` to 50,000 or 100,000\n",
        "   - Better performance needs more training!\n",
        "\n",
        "3. **Load and continue training**:\n",
        "   ```python\n",
        "   CONFIG[\"load_checkpoint\"] = \"checkpoints/notebook_quick_test/rl_model_20000_steps.zip\"\n",
        "   CONFIG[\"total_timesteps\"] = 50_000  # Train for 30k more steps\n",
        "   ```\n",
        "\n",
        "4. **Visualize attention patterns**:\n",
        "   ```python\n",
        "   # See what frames the transformer focuses on\n",
        "   attention_info = learning_agent.visualize_attention(obs)\n",
        "   ```\n",
        "\n",
        "5. **Compare experiments**:\n",
        "   - Run multiple training sessions with different configs\n",
        "   - Plot them together to compare\n",
        "\n",
        "6. **Read the full guide**:\n",
        "   - Check out `NOTEBOOK_TRAINING_GUIDE.md` for advanced tips\n",
        "\n",
        "---\n",
        "\n",
        "### Key Benefits You Just Experienced:\n",
        "\n",
        "‚úÖ **Easy Configuration**: Changed settings in one cell  \n",
        "‚úÖ **Interactive**: Ran evaluation and plotting without restarting  \n",
        "‚úÖ **Fast Iteration**: Quick feedback loop for experimentation  \n",
        "‚úÖ **Visual Feedback**: Saw training curves immediately  \n",
        "\n",
        "### When to use Scripts vs Notebooks:\n",
        "\n",
        "- **Notebooks**: Experimentation, debugging, short runs, learning\n",
        "- **Scripts** (`train_agent.py`): Long overnight runs, production, automation\n",
        "\n",
        "Happy training! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
