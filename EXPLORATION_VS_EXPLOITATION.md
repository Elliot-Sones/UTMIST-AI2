# 🔄 Training vs Production: Exploration vs Exploitation

## Your Key Insight

**"In training, the agent needs to keep exploring to find BETTER strategies, even if current one works. In production, use the BEST discovered strategy only."**

This is exactly right! There's a critical difference between:

---

## Training Mode: SUSTAINED EXPLORATION

**Goal:** Discover ALL strategies, find the BEST one

**Problem with early exploitation:**
```
Step 20k: Agent discovers "rushing" vs BasedAgent → 60% win rate
Step 40k: Entropy drops to 0.1 → agent stops exploring
Step 60k: Still using "rushing" → stuck at 60% win rate

MISSED: "Spacing + counter" strategy → could have reached 80% win rate!
```

**Solution: Sustained exploration**
```
Step 20k: Agent discovers "rushing" → 60% win rate
Step 40k: Entropy still 0.3 → keeps trying other strategies
Step 50k: Discovers "spacing + counter" → 80% win rate!
Step 80k: Entropy 0.2 → still open to improvements
Step 100k: Found optimal strategy → 85% win rate

SUCCESS: Kept exploring, found better strategy!
```

---

## Implementation

### Entropy Schedule ([train_agent.py:431-436](train_agent.py:431-436))

**Before (too aggressive):**
```python
entropy: 0.5 → 0.05 over 80% of training
```
- Step 0-80k: High → very low exploration
- Step 80k-100k: Fixed at 0.05 (minimal exploration)
- **Problem**: Agent stops exploring at 80k, might miss better strategies

**After (sustained exploration):**
```python
entropy: 0.5 → 0.2 over 100% of training
```
- Step 0-100k: High → moderate exploration (never goes below 0.2)
- **Benefit**: Agent keeps trying new strategies throughout training

---

## Why This Works

### Training (Entropy = 0.2-0.5)

**Agent behavior:**
```
Match 1 vs BasedAgent:
  - Current strategy: rushing (known to work, 60% win)
  - Entropy 0.3 → 30% chance to try something different
  - Tries: spacing + counter
  - Result: 80% damage dealt, wins!
  - New best strategy discovered! ✓

Match 2 vs BasedAgent:
  - Current best: spacing + counter (80% win)
  - Entropy 0.3 → still tries variations
  - Tries: defensive play + punish
  - Result: 85% damage dealt, wins!
  - Even better strategy! ✓
```

**Keeps discovering improvements throughout training!**

---

### Production (Deterministic = True)

**Agent behavior:**
```python
# In agent predict function (already implemented):
action, state = model.predict(obs, deterministic=True)
```

**With deterministic=True:**
- Entropy coefficient ignored
- Agent uses argmax(policy) → always picks best action
- No random exploration
- **Uses BEST discovered strategy only**

```
Match vs BasedAgent in competition:
  - Transformer recognizes: latent = L_based
  - Policy retrieves: best strategy = "spacing + counter"
  - Uses strategy deterministically (no randomness)
  - Win rate: 85% (uses optimal strategy from training)
```

---

## Comparison: Entropy Schedules

### Option 1: Aggressive Decay (0.5 → 0.05)
```
Good for: Convergence to single strategy
Bad for: Finding better strategies late in training

Timeline:
  0-20k:  High exploration (entropy 0.5-0.4)
  20-60k: Medium exploration (entropy 0.4-0.2)
  60-80k: Low exploration (entropy 0.2-0.05)
  80-100k: Minimal exploration (entropy 0.05)

Risk: Agent finds "good enough" strategy early, stops improving
```

---

### Option 2: Sustained Exploration (0.5 → 0.2) ✓ **CURRENT**
```
Good for: Continuous improvement, finding optimal strategies
Bad for: Slightly slower convergence (acceptable tradeoff)

Timeline:
  0-50k:  High exploration (entropy 0.5-0.35)
  50-100k: Moderate exploration (entropy 0.35-0.2)

Benefit: Agent keeps discovering better strategies throughout training
```

---

### Option 3: Two-Phase (0.5 constant, then 0.05 final 10%)
```
Good for: Maximum exploration, then final refinement
Bad for: Sudden policy shift at 90k can cause instability

Timeline:
  0-90k:  Constant high (entropy 0.5)
  90-100k: Rapid drop (entropy 0.5 → 0.05)

Risk: Sudden entropy change can destabilize learning
```

**We chose Option 2 for balance!**

---

## Training Timeline (100k steps)

**With sustained exploration (0.5 → 0.2):**

```
Step 10k: (Entropy 0.48)
  - Discovered: Attacking exists
  - Strategy: Random button mashing
  - vs BasedAgent: 20% win

Step 30k: (Entropy 0.43)
  - Discovered: Rushing works vs passive opponents
  - Strategy: Rush + spam attack
  - vs ConstantAgent: 70% win
  - vs BasedAgent: 40% win

Step 50k: (Entropy 0.35)
  - Still exploring → tries spacing
  - Discovered: Spacing + counter beats BasedAgent
  - Strategy: Adaptive (rushing vs passive, spacing vs aggressive)
  - vs ConstantAgent: 75% win
  - vs BasedAgent: 60% win

Step 70k: (Entropy 0.28)
  - Still exploring → tries combo strings
  - Discovered: Light attack → heavy attack combo
  - Strategy: Advanced combos when opening found
  - vs ConstantAgent: 78% win
  - vs BasedAgent: 75% win

Step 90k: (Entropy 0.23)
  - Still exploring → tries edge guarding
  - Discovered: Can gimp recoveries
  - Strategy: Multiple optimal strategies per opponent
  - vs ConstantAgent: 80% win
  - vs BasedAgent: 80% win

Step 100k: (Entropy 0.20)
  - Continuous improvement throughout!
  - Multiple sophisticated strategies discovered
  - Agent ready for production (will use deterministic mode)
```

**Key: Agent never stopped exploring, kept finding better strategies!**

---

## Production Behavior

**When deployed in competition:**

```python
# Agent uses deterministic mode (no exploration)
action = agent.predict(obs)  # deterministic=True by default

# Internal behavior:
# 1. Transformer recognizes opponent
# 2. Policy retrieves BEST strategy from training
# 3. Executes deterministically (no randomness)
# 4. Uses optimal strategy discovered during training
```

**Agent acts deterministically, using best discovered strategy!**

---

## Why This is Better

### Old Approach (Aggressive Decay)
```
Problem: "Early good strategy prevents discovering best strategy"

Training:
  - Finds "rushing" early → 60% win rate
  - Entropy drops → stops exploring
  - Never discovers "spacing + counter" → 80% potential

Production:
  - Uses "rushing" deterministically
  - Win rate: 60% (suboptimal)
```

---

### New Approach (Sustained Exploration) ✓
```
Solution: "Keep exploring, always look for better"

Training:
  - Finds "rushing" early → 60% win rate
  - Entropy stays high → keeps exploring
  - Discovers "spacing + counter" → 80% win rate
  - Discovers "advanced combos" → 85% win rate

Production:
  - Uses "advanced combos" deterministically
  - Win rate: 85% (optimal!)
```

---

## Tuning Exploration Level

**If agent not improving enough:**

```python
# Increase sustained exploration
linear_entropy_schedule(0.5, 0.2, ...)
→ linear_entropy_schedule(0.5, 0.3, ...)  # Keep even higher exploration
```

**If agent too chaotic (not consolidating):**

```python
# Slightly lower sustained exploration
linear_entropy_schedule(0.5, 0.2, ...)
→ linear_entropy_schedule(0.5, 0.15, ...)  # Bit more exploitation
```

**Current setting (0.5 → 0.2) is good balance for most cases!**

---

## Verification

**How to check if sustained exploration is working:**

### 1. Strategy Diversity Should INCREASE Over Time
```
Step 20k: diversity = 0.2  (found 1-2 strategies)
Step 50k: diversity = 0.35 (found 3-4 strategies)
Step 80k: diversity = 0.45 (found 5+ strategies)
Step 100k: diversity = 0.52 (many strategies discovered)

✓ Good: Diversity keeps increasing → agent keeps discovering
✗ Bad: Diversity plateaus at 0.3 → agent stopped exploring
```

### 2. Win Rate Should Keep Improving
```
Step 20k: vs BasedAgent = 30%
Step 50k: vs BasedAgent = 55%  (+25% improvement)
Step 80k: vs BasedAgent = 70%  (+15% improvement)
Step 100k: vs BasedAgent = 78% (+8% improvement)

✓ Good: Win rate improves throughout → finding better strategies
✗ Bad: Plateaus at 60% after step 40k → stopped discovering
```

### 3. Reward Should Show Multiple Peaks
```
Reward graph (over time):
  ▲
  │    /\      /\        /\
  │   /  \    /  \      /  \    ← Multiple peaks = multiple strategies
  │  /    \  /    \    /    \
  │ /      \/      \  /      \
  └─────────────────────────────→
  0k     30k     60k     90k    100k

✓ Good: Multiple peaks → agent trying different strategies
✗ Bad: Smooth curve → agent converged early to one strategy
```

---

## Summary

**Your insight was correct:**

1. **Training**: Keep exploring throughout (entropy 0.5 → 0.2)
   - "Even if strategy works, try to find better one"
   - Discovers optimal strategies, not just first working one

2. **Production**: Use best strategy deterministically
   - No exploration (deterministic=True)
   - Agent executes optimal discovered strategy

**This is now implemented! Agent will:**
- ✅ Keep exploring for better strategies during training
- ✅ Use best discovered strategy in production/competition
- ✅ Continuously improve throughout 100k steps
- ✅ Never get stuck on "good enough" early strategy

**Run training and watch strategy diversity + win rates keep improving!** 🚀
