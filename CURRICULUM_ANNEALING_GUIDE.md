# üéì CURRICULUM REWARD ANNEALING SYSTEM

**Purpose:** Gradually transition from exploration (high dense rewards) to exploitation (high sparse rewards) over training

**Problem Solved:** Sudden reward changes cause value function mismatch and training collapse

---

## üö® **WHY WE NEED THIS**

### **The Problem (What Happened at Step 30k)**

**Previous Approach (Sudden Change):**
```python
# Step 0-50k: High dense rewards
head_to_opponent: 10.0
on_attack_button_press: 15.0

# Step 50k: SUDDEN DROP to 3.0 (70-80% reduction)
head_to_opponent: 3.0   # ‚ùå Too aggressive!
on_attack_button_press: 3.0
```

**Result:**
- Agent regressed from 0.05 damage ratio ‚Üí 0.02 (WORSE!)
- Win rate dropped to 0% vs both opponents
- Value function trained on old weights couldn't adapt
- **Agent forgot how to attack**

### **The Solution (Gradual Annealing)**

```python
# Stage 1 (0-25k): Original weights
head_to_opponent: 10.0
on_attack_button_press: 15.0

# Stage 2 (25k-50k): First reduction (30-33% cut)
head_to_opponent: 7.0
on_attack_button_press: 10.0

# Stage 3 (50k-100k): Moderate reduction (50-60% cut)
head_to_opponent: 5.0
on_attack_button_press: 6.0

# Stage 4 (100k+): Conservative reduction (70-80% cut)
head_to_opponent: 3.0
on_attack_button_press: 3.0

# Stage 5 (200k+): Minimal guidance (90%+ cut)
head_to_opponent: 1.0
on_attack_button_press: 1.0
```

**Result:** Value function adapts gradually, no collapse!

---

## üìÖ **CURRICULUM SCHEDULE**

### **Complete Timeline**

| Stage | Steps | head_to_opponent | on_attack_button_press | Reduction | Phase |
|-------|-------|------------------|------------------------|-----------|-------|
| 1 | 0-25k | 10.0 | 15.0 | 0% (baseline) | Exploration |
| 2 | 25k-50k | 7.0 | 10.0 | 30-33% | Refinement |
| 3 | 50k-100k | 5.0 | 6.0 | 50-60% | Optimization |
| 4 | 100k-200k | 3.0 | 3.0 | 70-80% | Mastery |
| 5 | 200k+ | 1.0 | 1.0 | 90-93% | Competition |

### **Outcome Rewards (Fixed Throughout)**

These stay HIGH to encourage winning:
```python
damage_interaction_reward: 500.0  # Always high
on_win_reward: 2000               # Always high
on_knockout_reward: 150           # Always high
```

---

## üéØ **HOW IT WORKS**

### **Automatic Weight Updates**

The system updates reward weights **automatically** every training step:

1. **At Training Start:**
   - Curriculum schedule prints to console
   - Weights start at Stage 1 (10.0, 15.0)

2. **Every Training Step:**
   - Callback checks current step count
   - Compares to milestone thresholds
   - Updates reward weights if stage changed

3. **At Stage Transitions:**
   ```
   ======================================================================
   üéì CURRICULUM TRANSITION: Stage 2
      Step: 25,000
      head_to_opponent: 7.0
      on_attack_button_press: 10.0
   ======================================================================
   ```

### **Value Function Adaptation**

**Why gradual works:**
- Value function: V(s) = expected future reward
- Trained over many steps on current reward weights
- Sudden changes ‚Üí value estimates wrong ‚Üí poor decisions
- **Gradual changes ‚Üí value re-learns slowly ‚Üí stable**

---

## üìä **EXPECTED TRAINING CURVES**

### **Stage 1 (0-25k): Exploration**
```
Damage Ratio: 0.0 ‚Üí 0.2
Win Rate (Constant): 0% ‚Üí 40%
Reward Mix: 80% dense, 20% sparse
```
**What's happening:** Agent learning basic attacking

### **Stage 2 (25k-50k): Refinement**
```
Damage Ratio: 0.2 ‚Üí 0.8
Win Rate (Constant): 40% ‚Üí 70%
Reward Mix: 60% dense, 40% sparse
```
**What's happening:** Agent refining attack timing

### **Stage 3 (50k-100k): Optimization**
```
Damage Ratio: 0.8 ‚Üí 1.5
Win Rate (Constant): 70% ‚Üí 85%
Win Rate (Based): 10% ‚Üí 45%
Reward Mix: 40% dense, 60% sparse
```
**What's happening:** Agent optimizing for wins, not just damage

### **Stage 4 (100k-200k): Mastery**
```
Damage Ratio: 1.5 ‚Üí 2.5
Win Rate (Constant): 85% ‚Üí 95%
Win Rate (Based): 45% ‚Üí 70%
Reward Mix: 20% dense, 80% sparse
Transformer Variance: >0.25 (adapting!)
```
**What's happening:** Agent developing opponent-specific strategies

### **Stage 5 (200k+): Competition**
```
Win Rate (Constant): 95%+
Win Rate (Based): 75%+
Win Rate (Self-play): 45-55% (balanced)
Reward Mix: 10% dense, 90% sparse
```
**What's happening:** Agent mastered, ready for competition

---

## üîß **CONFIGURATION**

### **Current Schedule (in train_agent.py)**

Located at Line 1735-1741:
```python
class CurriculumRewardScheduler:
    def __init__(self):
        self.milestones = [
            (0,      10.0, 15.0),  # Stage 1: Initial exploration
            (25_000,  7.0, 10.0),  # Stage 2: First reduction
            (50_000,  5.0,  6.0),  # Stage 3: Moderate reduction
            (100_000, 3.0,  3.0),  # Stage 4: Conservative reduction
            (200_000, 1.0,  1.0),  # Stage 5: Minimal guidance
        ]
```

### **Customizing the Schedule**

**If agent learning too slowly (stuck at low win rate):**
```python
# Delay reductions - more time at each stage
self.milestones = [
    (0,      10.0, 15.0),
    (50_000,  7.0, 10.0),  # Was 25k, now 50k
    (100_000, 5.0,  6.0),  # Was 50k, now 100k
    (200_000, 3.0,  3.0),  # Was 100k, now 200k
    (400_000, 1.0,  1.0),  # Was 200k, now 400k
]
```

**If agent overfitting to dense rewards (reward hacking):**
```python
# Faster reductions - less time farming
self.milestones = [
    (0,      10.0, 15.0),
    (10_000,  7.0, 10.0),  # Earlier: 25k ‚Üí 10k
    (25_000,  5.0,  6.0),  # Earlier: 50k ‚Üí 25k
    (50_000,  3.0,  3.0),  # Earlier: 100k ‚Üí 50k
    (100_000, 1.0,  1.0),  # Earlier: 200k ‚Üí 100k
]
```

**If training for shorter runs (50k total):**
```python
# Compressed schedule for quick iteration
self.milestones = [
    (0,      10.0, 15.0),
    (12_500,  7.0, 10.0),  # 25% of 50k
    (25_000,  5.0,  6.0),  # 50% of 50k
    (37_500,  3.0,  3.0),  # 75% of 50k
    (50_000,  1.0,  1.0),  # 100% of 50k
]
```

---

## üìà **MONITORING CURRICULUM**

### **Watch for These Transitions**

**Stage 1 ‚Üí Stage 2 (Step 25,000):**
```
Expected:
  - Damage ratio should be >0.1 (agent can attack)
  - Win rate vs Constant >30%
  - on_attack_button_press still firing frequently

If NOT met:
  - Delay Stage 2 transition to 35k or 50k
```

**Stage 2 ‚Üí Stage 3 (Step 50,000):**
```
Expected:
  - Damage ratio >0.5
  - Win rate vs Constant >60%
  - damage_interaction_reward firing regularly

If NOT met:
  - Keep Stage 2 weights for 25k more steps
```

**Stage 3 ‚Üí Stage 4 (Step 100,000):**
```
Expected:
  - Damage ratio >1.0 (winning trades)
  - Win rate vs Constant >80%
  - Win rate vs Based >35%
  - Sparse rewards >60% of total

If NOT met:
  - Assessment will show FIX_REQUIRED
  - Stay at Stage 3 longer
```

---

## üéì **TRAINING RECOMMENDATIONS**

### **For 50k Quick Validation:**
Use compressed schedule (see above) to test full curriculum in 50k steps

### **For 100k Pre-Competition:**
Use current schedule - reaches Stage 4 (mastery) at 100k

### **For 10M Competition:**
Use current schedule - benefits of early exploration + late optimization
- 0-200k: All 5 stages complete
- 200k-10M: Stage 5 (minimal guidance, transformer-driven)

---

## üö® **TROUBLESHOOTING**

### **Problem: Agent Still Regresses at Transitions**

**Symptoms:** Win rate drops when moving to next stage

**Fix:** Smooth the transitions with intermediate steps:
```python
# Instead of: 10.0 ‚Üí 7.0 (sudden)
# Use: 10.0 ‚Üí 8.5 ‚Üí 7.0 (gradual)

self.milestones = [
    (0,      10.0, 15.0),
    (20_000,  8.5, 12.5),  # ‚Üê Add intermediate
    (25_000,  7.0, 10.0),
    # ... etc
]
```

### **Problem: Agent Reaches 200k But Still Failing**

**Symptoms:** Even at Stage 5 (1.0 weights), agent not winning

**Root Cause:** Agent never learned fundamentals properly

**Fix:** Restart with LONGER Stage 1:
```python
self.milestones = [
    (0,      10.0, 15.0),
    (100_000,  7.0, 10.0),  # 4x longer exploration!
    # ... etc
]
```

### **Problem: Transformer Still Not Used (Variance <0.2)**

**Symptoms:** Latent norm not varying even at Stage 5

**Fix:** Check opponent diversity:
```python
# If only training vs ConstantAgent:
"opponent_mix": {
    "constant_agent": (0.3, partial(ConstantAgent)),
    "based_agent": (0.4, partial(BasedAgent)),      # Add variety!
    "self_play": (0.3, None),                       # Force adaptation
}
```

---

## ‚úÖ **SUCCESS CRITERIA BY STAGE**

### **Stage 1 Complete (25k):**
- [ ] Damage ratio >0.1
- [ ] Win rate vs Constant >30%
- [ ] Agent consistently attacking (not passive)

### **Stage 2 Complete (50k):**
- [ ] Damage ratio >0.5
- [ ] Win rate vs Constant >65%
- [ ] Sparse rewards >40% of total

### **Stage 3 Complete (100k):**
- [ ] Damage ratio >1.0
- [ ] Win rate vs Constant >80%
- [ ] Win rate vs Based >35%
- [ ] Sparse rewards >60% of total

### **Stage 4 Complete (200k):**
- [ ] Damage ratio >1.8
- [ ] Win rate vs Constant >90%
- [ ] Win rate vs Based >60%
- [ ] Transformer variance >0.20
- [ ] Sparse rewards >75% of total
- [ ] **READY FOR 10M SCALE**

---

## üìû **QUICK REFERENCE**

**Check current stage:**
- Watch console for "üéì CURRICULUM TRANSITION" messages
- Appears at steps: 25k, 50k, 100k, 200k

**Verify weights updating:**
```bash
# In reward_breakdown.csv, check weights changing:
tail -100 checkpoints/.../reward_breakdown.csv | grep "head_to_opponent"
# Should see values decreasing over time
```

**Force manual weight:**
```python
# In gen_reward_manager(), temporarily override:
head_weight = 5.0  # Force specific weight for debugging
```

---

## üéØ **NEXT STEPS**

1. **Start 100k training** with curriculum system
2. **Monitor transitions** at 25k, 50k, 100k
3. **Check assessment** at 100k checkpoint
4. **If READY:** Scale to 10M competition training
5. **If NOT READY:** Adjust schedule and retrain

**The curriculum system gives you ~90% confidence for scaling by gradually adapting the agent instead of shocking it!**
