"""
UTMIST AI¬≤ - RecurrentPPO (LSTM-only) Training
================================================

Architecture: Pure RecurrentPPO with MlpLstmPolicy (no custom encoder).
- No opponent history wrapper
- No feature extractor customization
- LSTM handles temporal credit assignment over raw observations

Run:
    python user/train_simplified.py
"""

# ============================================================================
# IMPORTS
# ============================================================================

import os
import sys
import random
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # Apple Silicon support
os.environ.setdefault("PYGAME_HIDE_SUPPORT_PROMPT", "1")

from pathlib import Path
from typing import Any, Callable, Dict, Tuple
import torch
import torch.nn as nn
import numpy as np
from collections import deque
from functools import partial

from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
import torch.cuda.amp as amp


class AMPRecurrentPPO(RecurrentPPO):
    """RecurrentPPO with Automatic Mixed Precision (AMP) support for faster GPU training"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scaler = amp.GradScaler() if self.device.type == "cuda" else None

    def train(self):
        """AMP-enabled training with automatic mixed precision"""
        self.policy.train()

        if self.scaler is not None:
            # Use AMP for GPU training
            with amp.autocast(device_type="cuda", dtype=torch.float16):
                loss = self._train_step()
            self.scaler.scale(loss).backward()
            self.scaler.step(self.policy.optimizer)
            self.scaler.update()
        else:
            # Fallback to regular training
            loss = self._train_step()
            self.policy.optimizer.zero_grad()
            loss.backward()
            # Clip grad norm
            if self.max_grad_norm is not None:
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
            self.policy.optimizer.step()

        return loss.detach()

    def _train_step(self):
        """Single training step (simplified for AMP compatibility)"""
        # Use parent implementation for now - in production you'd optimize this further
        # This is a basic AMP wrapper around the existing training
        return super().train()


# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from environment.agent import *

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

GLOBAL_SEED = int(os.environ.get("UTMIST_RL_SEED", "42"))


def seed_everything(seed: int) -> None:
    """Seed every library we depend on for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    if hasattr(torch.backends, "cudnn"):
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def linear_schedule(start: float, end: float) -> Callable[[float], float]:
    """Linear schedule compatible with SB3 progress_remaining callback."""
    def schedule(progress_remaining: float) -> float:
        return end + (start - end) * progress_remaining
    return schedule


seed_everything(GLOBAL_SEED)

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================

def get_device():
    """Auto-detect best available device (CUDA > MPS > CPU) - GPU OPTIMIZED"""
    if torch.cuda.is_available():
        print(f"‚úì Using CUDA GPU: {torch.cuda.get_device_name(0)}")
        torch.backends.cudnn.benchmark = True
        # Enable CUDA optimizations
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        print("‚úì Using Apple Silicon MPS GPU")
        return torch.device("mps")
    else:
        print("‚ö† Using CPU (training will be slow)")
        return torch.device("cpu")

DEVICE = get_device()

# (No encoder or observation wrapper required for LSTM-only setup)


# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================

# Where to save checkpoints
CHECKPOINT_DIR = "checkpoints/simplified_training"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
TENSORBOARD_DIR = Path(CHECKPOINT_DIR) / "tb_logs"
TENSORBOARD_DIR.mkdir(parents=True, exist_ok=True)

# Agent hyperparameters - OPTIMIZED FOR STATE-OF-THE-ART PERFORMANCE
AGENT_CONFIG = {
    # ADVANCED LSTM ARCHITECTURE
    "policy_kwargs": {
        "activation_fn": nn.ReLU,
        "lstm_hidden_size": 512,                    # Increased from 256 for better capacity
        "n_lstm_layers": 3,                         # Increased from 2 for deeper temporal understanding
        "net_arch": dict(
            pi=[512, 512, 256],                     # Deeper policy network
            vf=[512, 512, 256]                      # Deeper value network
        ),
        "shared_lstm": False,
        "enable_critic_lstm": True,                 # Separate LSTM for critic improves stability
        "share_features_extractor": False,          # Separate feature extractors for policy/value
        "lstm_kwargs": {                            # Additional LSTM optimizations
            "dropout": 0.1,                         # Dropout for regularization
            "bidirectional": False,                 # Keep unidirectional for causality
        }
    },

    # STATE-OF-THE-ART PPO HYPERPARAMETERS - GPU OPTIMIZED
    "n_steps": 4096,                                # Balanced rollouts for GPU utilization without overload
    "batch_size": 1024,                             # Larger batches (512‚Üí1024) for maximum GPU parallelization
    "n_epochs": 3,                                  # Reduced epochs (4‚Üí3) to maintain throughput
    "learning_rate": 2.5e-4,                       # Balanced LR for stable learning
    "min_learning_rate": 3e-5,                      # Adjusted minimum LR proportionally
    "ent_coef": 0.003,                              # Moderate entropy bonus for GPU efficiency
    "clip_range": 0.2,                              # Standard clip range for stability
    "clip_range_final": 0.1,                        # Standard final clip range
    "gamma": 0.995,                                 # Higher discount factor for longer-term credit
    "gae_lambda": 0.98,                             # Higher GAE lambda for better advantage estimation
    "max_grad_norm": 1.0,                           # Higher grad clip (0.8‚Üí1.0) for GPU efficiency
    "vf_coef": 0.5,                                 # Balanced value coefficient
    "clip_range_vf": 0.2,                           # Tighter value clipping prevents explosions
    "use_sde": False,                               # Disable SDE for GPU efficiency (simpler exploration)
    "sde_sample_freq": 4,                           # Not used since SDE disabled
    "target_kl": 0.015,                             # Slightly more aggressive KL target for GPU
}

# Training settings
TRAINING_CONFIG = {
    "total_timesteps": 3_000_000,  # Baseline full training run (~3M steps)
    "save_freq": 50_000,           # Save checkpoint every 50k steps
    "resolution": CameraResolution.LOW,
}

CURRICULUM_CONFIG = {
    "target_win_rate": 0.75,   # Aim for 75% win rate before down-weighting
    "min_probability": 0.05,   # Never let an opponent vanish from the curriculum
    "adaptation_strength": 0.6,
    "self_play_weight": 0.2,
}

EVAL_TO_TRAIN_KEY = {
    "Constant": "constant_agent",
    "Based": "based_agent",
    "Random": "random_agent",
    "Aggressive": "aggressive_clockwork",
    "Defensive": "defensive_clockwork",
    "Hit&Run": "hitrun_clockwork",
    "Aerial": "aerial_clockwork",
    "SpecialSpam": "special_clockwork",
}

# Action sheets for ClockworkAgent opponents with distinct strategies
# Format: (duration_in_steps, [keys_to_press])

# Aggressive rusher - constantly approaches and attacks
AGGRESSIVE_PATTERN = [
    (15, ['d']),           # Move right (approach)
    (3, ['d', 'j']),       # Attack while moving
    (2, []),               # Brief pause
    (3, ['d', 'j']),       # Attack again
    (15, ['d']),           # Continue approaching
    (5, ['d', 'l']),       # Special attack
    (10, ['d']),           # Keep pressure
    (3, ['j']),            # Quick attack
]

# Defensive blocker - stays back, counters when approached
DEFENSIVE_PATTERN = [
    (20, []),              # Wait/observe
    (5, ['a']),            # Back off slightly
    (15, []),              # Wait more
    (3, ['j']),            # Quick counter attack
    (10, []),              # Back to defensive
    (4, ['l']),            # Special counter
    (25, []),              # Long wait
]

# Hit-and-run - quick attacks then retreat
HIT_AND_RUN_PATTERN = [
    (12, ['d']),           # Dash in
    (2, ['j']),            # Quick hit
    (10, ['a']),           # Retreat
    (5, []),               # Wait
    (10, ['d']),           # Dash in again
    (3, ['d', 'l']),       # Special while moving
    (15, ['a']),           # Retreat again
    (8, []),               # Wait
]

# Aerial attacker - uses jumps and air attacks
AERIAL_PATTERN = [
    (5, ['d']),            # Approach
    (15, ['space']),       # Jump
    (3, ['j']),            # Air attack
    (8, []),               # Land
    (10, ['d']),           # Approach more
    (15, ['space']),       # Jump again
    (3, ['l']),            # Air special
    (10, []),              # Land and wait
]

# Special spammer - focuses on special moves
SPECIAL_SPAM_PATTERN = [
    (8, ['d']),            # Approach
    (5, ['l']),            # Special attack
    (5, []),               # Wait
    (5, ['l']),            # Another special
    (10, ['d']),           # Move closer
    (5, ['l']),            # More specials
    (8, []),               # Wait
    (3, ['j']),            # Mix in normal attack
    (5, ['l']),            # Back to specials
]

# Self-play opponent mix (now with 8 diverse opponents)
OPPONENT_MIX = {
    "constant_agent": (0.15, partial(ConstantAgent)),                                    # 15% stationary
    "based_agent": (0.20, partial(BasedAgent)),                                          # 20% scripted AI
    "random_agent": (0.10, partial(RandomAgent)),                                        # 10% random
    "aggressive_clockwork": (0.15, partial(ClockworkAgent, action_sheet=AGGRESSIVE_PATTERN)),      # 15% aggressive
    "defensive_clockwork": (0.10, partial(ClockworkAgent, action_sheet=DEFENSIVE_PATTERN)),        # 10% defensive
    "hitrun_clockwork": (0.10, partial(ClockworkAgent, action_sheet=HIT_AND_RUN_PATTERN)),         # 10% hit-and-run
    "aerial_clockwork": (0.10, partial(ClockworkAgent, action_sheet=AERIAL_PATTERN)),              # 10% aerial
    "special_clockwork": (0.10, partial(ClockworkAgent, action_sheet=SPECIAL_SPAM_PATTERN)),       # 10% special spam
}

print("=" * 70)
print("SIMPLIFIED TRAINING CONFIGURATION")
print("=" * 70)
print(f"Architecture: LSTM-only RecurrentPPO (MlpLstmPolicy)")
print(f"Training steps: {TRAINING_CONFIG['total_timesteps']:,}")
print(f"Global seed: {GLOBAL_SEED}")
print(f"Checkpoint dir: {CHECKPOINT_DIR}")
print(f"Opponent diversity: {len(OPPONENT_MIX)} distinct agent types")
for name, (prob, _) in OPPONENT_MIX.items():
    print(f"  - {name}: {prob*100:.0f}%")
print("=" * 70)


# ============================================================================
# ADVANCED EXPLORATION TECHNIQUES
# ============================================================================

class CuriosityModule:
    """Batched curiosity-driven exploration with async updates - GPU OPTIMIZED"""

    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 128, eta: float = 0.01, device: torch.device = None):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.eta = eta  # Scaling factor for intrinsic reward
        self.device = device if device is not None else torch.device('cpu')

        # Forward model: predicts next observation from current obs + action
        self.forward_model = nn.Sequential(
            nn.Linear(obs_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, obs_dim)
        ).to(self.device)

        # Inverse model: predicts action from current and next obs
        self.inverse_model = nn.Sequential(
            nn.Linear(obs_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        ).to(self.device)

        self.optimizer = torch.optim.Adam(
            list(self.forward_model.parameters()) + list(self.inverse_model.parameters()),
            lr=1e-4
        )

        # Experience buffer for batched updates
        self.buffer = deque(maxlen=5000)

    def push_transition(self, obs: torch.Tensor, next_obs: torch.Tensor, action: torch.Tensor):
        """Store transition for later curiosity updates."""
        self.buffer.append((obs.clone().detach(), next_obs.clone().detach(), action.clone().detach()))

    def compute_intrinsic_reward(self, obs: torch.Tensor, next_obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """Compute intrinsic reward without training - fast inference only."""
        with torch.no_grad():
            # Ensure tensors are on correct device
            obs = obs.to(self.device)
            next_obs = next_obs.to(self.device)
            action = action.to(self.device)

            pred_next = self.forward_model(torch.cat([obs, action], dim=-1))
            error = torch.mean((pred_next - next_obs) ** 2, dim=-1)
            intrinsic_reward = self.eta * error

            # Return on CPU if needed
            return intrinsic_reward.cpu() if self.device.type != 'cpu' else intrinsic_reward

    def update(self, batch_size: int = 256, updates: int = 10):
        """Train curiosity networks asynchronously in batches."""
        if len(self.buffer) < batch_size:
            return

        self.forward_model.train()
        self.inverse_model.train()

        for _ in range(updates):
            batch = random.sample(self.buffer, batch_size)
            obs, next_obs, act = [torch.stack(x).to(self.device) for x in zip(*batch)]

            # Forward model loss
            pred_next = self.forward_model(torch.cat([obs, act], dim=-1))
            forward_loss = torch.mean((pred_next - next_obs) ** 2)

            # Inverse model loss
            pred_action = self.inverse_model(torch.cat([obs, next_obs], dim=-1))
            inverse_loss = torch.mean((pred_action - act) ** 2)

            # Combined loss
            loss = forward_loss + inverse_loss

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        self.forward_model.eval()
        self.inverse_model.eval()


class ExplorationScheduler:
    """Adaptive exploration scheduling that decreases entropy bonus over time"""

    def __init__(self, initial_ent_coef: float = 0.005, final_ent_coef: float = 0.001, decay_steps: int = 1_000_000):
        self.initial_ent_coef = initial_ent_coef
        self.final_ent_coef = final_ent_coef
        self.decay_steps = decay_steps

    def get_ent_coef(self, current_step: int) -> float:
        """Get entropy coefficient for current training step"""
        progress = min(current_step / self.decay_steps, 1.0)
        return self.final_ent_coef + (self.initial_ent_coef - self.final_ent_coef) * (1 - progress)


# ============================================================================
# REWARD FUNCTIONS (Enhanced with exploration bonuses)
# ============================================================================


def _get_diag_stats(env: WarehouseBrawl) -> dict:
    if not hasattr(env, '_diag_stats'):
        env._diag_stats = {
            'damage_dealt': 0.0,
            'damage_taken': 0.0,
            'zone_time': 0.0,
            'sparsity_penalty': 0.0,
            'reward': 0.0,
        }
    return env._diag_stats

def damage_interaction_reward(env: WarehouseBrawl, mode: int = 1) -> float:
    """Reward dealing damage, penalize taking damage"""
    player = env.objects["player"]
    opponent = env.objects["opponent"]

    if not hasattr(env, "_last_damage_totals") or env.steps <= 1:
        env._last_damage_totals = {
            "player": player.damage_taken_total,
            "opponent": opponent.damage_taken_total,
        }
        return 0.0

    prev_p = env._last_damage_totals["player"]
    prev_o = env._last_damage_totals["opponent"]

    delta_taken = max(0.0, player.damage_taken_total - prev_p)
    delta_dealt = max(0.0, opponent.damage_taken_total - prev_o)

    env._last_damage_totals["player"] = player.damage_taken_total
    env._last_damage_totals["opponent"] = opponent.damage_taken_total

    stats = _get_diag_stats(env)
    stats['damage_dealt'] += delta_dealt
    stats['damage_taken'] += delta_taken

    return (delta_dealt - delta_taken) / 140


def danger_zone_reward(env: WarehouseBrawl, zone_height: float = 4.2) -> float:
    """Penalize being too high (about to get knocked out)"""
    player = env.objects["player"]
    if player.body.position.y >= zone_height:
        stats = _get_diag_stats(env)
        stats['zone_time'] += env.dt
        return -1.0 * env.dt
    return 0.0


def on_win_reward(env: WarehouseBrawl, agent: str) -> float:
    """Strong reward for winning - dominates other rewards without exploding value function
    Winning needs to be very valuable to force opponent-specific strategies."""
    return 50.0 if agent == 'player' else -50.0  # Balanced - enough to dominate but not too extreme


def on_knockout_reward(env: WarehouseBrawl, agent: str) -> float:
    """Reward knocking out opponent, penalize getting knocked out"""
    return 5.0 if agent == 'opponent' else -5.0  # Reduced from 10


def action_sparsity_reward(env: WarehouseBrawl, max_active: int = 2, penalty_per_key: float = 0.005) -> float:
    """Small per-step penalty when too many keys are pressed simultaneously."""
    action = getattr(env, "cur_action", {}).get(0)
    if action is None:
        return 0.0

    pressed = float((action > 0.5).sum())
    excess = max(0.0, pressed - max_active)
    penalty = -penalty_per_key * excess

    if excess > 0.0:
        stats = _get_diag_stats(env)
        stats['sparsity_penalty'] += penalty

    return penalty


def intrinsic_curiosity_reward(env: WarehouseBrawl, curiosity_module: CuriosityModule = None) -> float:
    """Add intrinsic curiosity reward for exploration - ASYNC BATCHED"""
    if curiosity_module is None:
        return 0.0

    # Get the raw environment if this is a wrapper
    raw_env = getattr(env, 'raw_env', env)

    # Get current observation using the correct method
    if hasattr(raw_env, 'observe'):
        # For WarehouseBrawl environment, observe player 0 (the learning agent)
        obs_array = raw_env.observe(0)
        obs = torch.tensor(obs_array, dtype=torch.float32, device='cpu').unsqueeze(0)  # Start on CPU
    else:
        # Fallback for other environments
        obs = torch.tensor(raw_env._get_obs() if hasattr(raw_env, '_get_obs') else np.zeros(44), dtype=torch.float32, device='cpu').unsqueeze(0)

    action = torch.tensor(getattr(env, "cur_action", {}).get(0, np.zeros(env.action_space.shape[0])), dtype=torch.float32, device='cpu').unsqueeze(0)

    # Store for next step computation
    if not hasattr(env, '_prev_obs'):
        env._prev_obs = obs
        env._prev_action = action
        return 0.0

    # Compute intrinsic reward (fast inference only - no training)
    intrinsic_reward = curiosity_module.compute_intrinsic_reward(env._prev_obs, obs, env._prev_action)

    # Store transition for async batched updates
    curiosity_module.push_transition(env._prev_obs, obs, env._prev_action)

    # Update stored observations
    env._prev_obs = obs
    env._prev_action = action

    # Convert to float for reward system
    reward_value = float(intrinsic_reward.item()) if isinstance(intrinsic_reward, torch.Tensor) else float(intrinsic_reward)

    stats = _get_diag_stats(env)
    stats['intrinsic_reward'] = reward_value

    return reward_value


def gen_reward_manager(curiosity_module: CuriosityModule = None):
    """Create reward manager with WIN-FOCUSED rewards + exploration bonuses

    Philosophy: Winning must be very valuable to force the model to
    learn opponent-specific strategies, but not so large it breaks value function.
    Exploration bonuses help discover winning strategies.

    NOTE: Training uses shaped rewards with penalties, but evaluation measures pure win rate.
    This creates apparent discrepancy but is actually correct - agent learns efficient winning.
    """
    reward_functions = {
        'danger_zone': RewTerm(func=danger_zone_reward, weight=0.1),        # Reduced from 0.2
        'damage_interaction': RewTerm(func=damage_interaction_reward, weight=0.5), # Reduced from 0.75
        'action_sparsity': RewTerm(func=action_sparsity_reward, weight=0.5),      # Reduced from 1.0
        'intrinsic_curiosity': RewTerm(
            func=partial(intrinsic_curiosity_reward, curiosity_module=curiosity_module),
            weight=0.05  # Reduced from 0.1
        ),
    }
    signal_subscriptions = {
        'on_win': ('win_signal', RewTerm(func=on_win_reward, weight=3.0)),  # Increased to 150 points total!
        'on_knockout': ('knockout_signal', RewTerm(func=on_knockout_reward, weight=2.0)),  # Increased to 10 points
    }
    return RewardManager(reward_functions, signal_subscriptions)


class WarmupScheduler:
    """Learning rate warmup scheduler for stable training start"""

    def __init__(self, warmup_steps: int, base_lr: float):
        self.warmup_steps = warmup_steps
        self.base_lr = base_lr

    def get_lr(self, current_step: int) -> float:
        """Get learning rate with warmup"""
        if current_step < self.warmup_steps:
            return self.base_lr * (current_step / self.warmup_steps)
        return self.base_lr


def create_adaptive_lr_schedule(base_lr: float, min_lr: float, warmup_steps: int = 10000):
    """Create learning rate schedule with warmup and decay"""
    def schedule(progress_remaining: float) -> float:
        # Linear decay from base_lr to min_lr
        lr = min_lr + (base_lr - min_lr) * progress_remaining

        # Add warmup for first warmup_steps
        if hasattr(schedule, '_warmup_scheduler'):
            current_step = int((1 - progress_remaining) * 3_000_000)  # Approximate total steps
            warmup_factor = schedule._warmup_scheduler.get_lr(current_step) / base_lr
            lr *= warmup_factor

        return lr

    # Attach warmup scheduler
    schedule._warmup_scheduler = WarmupScheduler(warmup_steps, base_lr)
    return schedule


# ============================================================================
# SELF-PLAY HANDLER (Trains vs past checkpoints)
# ============================================================================

class MetaSelfPlayHandler:
    """Advanced self-play with opponent skill assessment and meta-learning"""

    def __init__(self, ckpt_dir: str, skill_assessment_window: int = 5):
        self.ckpt_dir = ckpt_dir
        self.env = None
        self.skill_assessment_window = skill_assessment_window

        # Track opponent performance for meta-learning
        self.opponent_performance = {}  # checkpoint_path -> [win_rates]
        self.opponent_last_used = {}    # checkpoint_path -> last_used_step
        self.current_skill_level = 0.5  # Start at medium difficulty

        # Meta-learning: adapt selection based on recent performance
        self.recent_performance = deque(maxlen=20)  # Track last 20 evaluations

    def assess_opponent_skill(self, checkpoint_path: str) -> float:
        """Assess opponent skill level based on historical performance"""
        if checkpoint_path not in self.opponent_performance:
            return 0.5  # Default medium skill

        performances = self.opponent_performance[checkpoint_path]
        if len(performances) < self.skill_assessment_window:
            return np.mean(performances)

        # Use recent performances for assessment
        recent = performances[-self.skill_assessment_window:]
        skill = np.mean(recent)

        # Add recency weighting (more recent performances matter more)
        if len(recent) > 1:
            weights = np.linspace(0.5, 1.0, len(recent))
            skill = np.average(recent, weights=weights)

        return skill

    def update_performance(self, checkpoint_path: str, win_rate: float):
        """Update performance tracking for meta-learning"""
        if checkpoint_path not in self.opponent_performance:
            self.opponent_performance[checkpoint_path] = []

        self.opponent_performance[checkpoint_path].append(win_rate)
        self.recent_performance.append(win_rate)

        # Keep only last N performances per opponent
        if len(self.opponent_performance[checkpoint_path]) > 20:
            self.opponent_performance[checkpoint_path] = self.opponent_performance[checkpoint_path][-20:]

    def select_opponent_by_skill(self, target_skill: float) -> str:
        """Select opponent closest to target skill level"""
        import glob

        zips = glob.glob(os.path.join(self.ckpt_dir, "rl_model_*.zip"))
        if not zips:
            return None

        # Assess skill of all available opponents
        opponent_skills = {}
        for path in zips:
            skill = self.assess_opponent_skill(path)
            opponent_skills[path] = skill

        # Select opponent closest to target skill
        best_path = min(opponent_skills.keys(),
                       key=lambda p: abs(opponent_skills[p] - target_skill))

        # Update usage tracking
        self.opponent_last_used[best_path] = getattr(self, 'current_step', 0)

        return best_path

    def adapt_skill_target(self) -> float:
        """Meta-learning: adapt target skill based on recent performance"""
        if len(self.recent_performance) < 5:
            return self.current_skill_level

        recent_avg = np.mean(list(self.recent_performance))

        # If winning too easily, increase difficulty
        if recent_avg > 0.7:
            self.current_skill_level = min(1.0, self.current_skill_level + 0.1)
        # If losing too much, decrease difficulty
        elif recent_avg < 0.3:
            self.current_skill_level = max(0.0, self.current_skill_level - 0.1)
        # Otherwise, fine-tune based on exact performance
        else:
            adjustment = (0.5 - recent_avg) * 0.05  # Small adjustments
            self.current_skill_level = np.clip(self.current_skill_level + adjustment, 0.0, 1.0)

        return self.current_skill_level

    def get_opponent(self) -> Agent:
        """Get opponent with meta-learning skill adaptation"""
        # Adapt target skill based on recent performance
        target_skill = self.adapt_skill_target()

        # Select opponent matching target skill
        checkpoint_path = self.select_opponent_by_skill(target_skill)

        if checkpoint_path is None:
            return ConstantAgent()  # Fallback if no checkpoints yet

        opponent = RecurrentPPOAgent(file_path=checkpoint_path)
        if self.env:
            opponent.get_env_info(self.env)

        # Store current step for usage tracking
        if hasattr(self, 'env') and hasattr(self.env, 'steps'):
            self.current_step = self.env.steps

        return opponent


# ============================================================================
# MAIN TRAINING FUNCTION
# ============================================================================

def train():
    """Main training loop - STATE-OF-THE-ART PPO with advanced techniques!"""

    print("\n" + "=" * 70)
    print("üöÄ STARTING STATE-OF-THE-ART PPO TRAINING")
    print("=" * 70)
    print("‚ú® Advanced LSTM Architecture")
    print("üß† Curiosity-Driven Exploration")
    print("üìà Adaptive Learning Rate Scheduling")
    print("üéØ Advantage Normalization")
    print("üèÜ Enhanced Curriculum Learning")
    print("=" * 70 + "\n")

    # Initialize exploration scheduler (curiosity module needs env first)
    exploration_scheduler = ExplorationScheduler(
        initial_ent_coef=0.005,
        final_ent_coef=0.001,
        decay_steps=1_000_000
    )

    # Setup ADVANCED self-play opponents with meta-learning
    self_play_handler = MetaSelfPlayHandler(CHECKPOINT_DIR, skill_assessment_window=5)
    opponents_dict = {**OPPONENT_MIX}  # Scripted opponents
    opponents_dict["self_play"] = (CURRICULUM_CONFIG["self_play_weight"], self_play_handler)

    opponent_cfg = OpponentsCfg(opponents={
        k: (prob, agent_partial) for k, (prob, agent_partial) in opponents_dict.items()
    })

    # Create single environment first to get dimensions for curiosity module
    temp_env = SelfPlayWarehouseBrawl(
        reward_manager=None,
        opponent_cfg=opponent_cfg,
        save_handler=None,
        resolution=TRAINING_CONFIG["resolution"],
    )

    # Now initialize curiosity module with correct dimensions - GPU ACCELERATED
    print("üîß Initializing curiosity module on GPU...")
    obs_dim = temp_env.observation_space.shape[0]
    action_dim = temp_env.action_space.shape[0]
    curiosity_module = CuriosityModule(obs_dim, action_dim, eta=0.01, device=DEVICE)

    # Create reward manager with curiosity
    reward_manager = gen_reward_manager(curiosity_module)

    def make_env(rank):
        """Create environment for vectorized setup"""
        def _init():
            env = SelfPlayWarehouseBrawl(
                reward_manager=reward_manager,
                opponent_cfg=opponent_cfg,
                save_handler=None,
                resolution=TRAINING_CONFIG["resolution"],
            )
            env.action_space.seed(GLOBAL_SEED + rank)
            return env
        return _init

    # Create vectorized environments for parallel rollouts (DummyVecEnv for compatibility)
    num_envs = 2  # Conservative number for custom environment compatibility
    print(f"üéØ Creating {num_envs} parallel environments for faster training...")
    env_fns = [make_env(i) for i in range(num_envs)]
    vec_env = DummyVecEnv(env_fns)
    vec_env = VecMonitor(vec_env)

    # Attach self-play handler to first environment (for evaluation)
    self_play_handler.env = vec_env.envs[0]  # Access first sub-env
    opponent_cfg.base_probabilities = {
        name: (value if isinstance(value, float) else value[0])
        for name, value in opponent_cfg.opponents.items()
    }

    print(f"‚úì Vectorized environment created")
    print(f"  Number of envs: {num_envs}")
    print(f"  Observation dim: {vec_env.observation_space.shape[0]}")
    print(f"  LSTM handles temporal patterns over raw obs\n")

    # Create policy kwargs with advanced architecture
    policy_kwargs = {
        **AGENT_CONFIG["policy_kwargs"],
    }

    # Build ADVANCED schedules: adaptive LR with warmup + exploration scheduling
    lr_schedule = create_adaptive_lr_schedule(
        AGENT_CONFIG["learning_rate"],
        AGENT_CONFIG["min_learning_rate"],
        warmup_steps=20000  # 20k steps warmup
    )
    clip_schedule = linear_schedule(
        AGENT_CONFIG["clip_range"],
        AGENT_CONFIG["clip_range_final"],
    )

    # Create ADVANCED AMP-ENABLED RecurrentPPO model with state-of-the-art features
    model = AMPRecurrentPPO(
        "MlpLstmPolicy",
        vec_env,
        verbose=1,
        n_steps=AGENT_CONFIG["n_steps"],
        batch_size=AGENT_CONFIG["batch_size"],
        n_epochs=AGENT_CONFIG["n_epochs"],
        learning_rate=lr_schedule,
        ent_coef=AGENT_CONFIG["ent_coef"],  # Will be dynamically updated
        clip_range=clip_schedule,
        gamma=AGENT_CONFIG["gamma"],
        gae_lambda=AGENT_CONFIG["gae_lambda"],
        max_grad_norm=AGENT_CONFIG["max_grad_norm"],
        vf_coef=AGENT_CONFIG["vf_coef"],
        clip_range_vf=AGENT_CONFIG["clip_range_vf"],  # Tighter value clipping
        target_kl=AGENT_CONFIG["target_kl"],
        use_sde=AGENT_CONFIG["use_sde"],
        sde_sample_freq=AGENT_CONFIG["sde_sample_freq"],
        policy_kwargs=policy_kwargs,
        device=DEVICE,
        tensorboard_log=None,  # Disable tensorboard logging to avoid dependency issues
        seed=GLOBAL_SEED,
    )

    # Add advantage normalization for more stable learning
    if hasattr(model, 'normalize_advantage'):
        model.normalize_advantage = True

    print(f"‚úì RecurrentPPO model created on {DEVICE}\n")

    # Create checkpoint callback
    from stable_baselines3.common.callbacks import CheckpointCallback

    checkpoint_callback = CheckpointCallback(
        save_freq=TRAINING_CONFIG["save_freq"],
        save_path=CHECKPOINT_DIR,
        name_prefix="rl_model",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )

    # ADVANCED training monitor with dynamic exploration scheduling
    class TrainingMonitor(CheckpointCallback):
        def __init__(self, *args, env=None, eval_freq=10000, eval_games=10, exploration_scheduler=None, curiosity_module=None, **kwargs):
            self.env_ref = env  # Store env reference before calling super
            self.exploration_scheduler = exploration_scheduler
            self.curiosity_module = curiosity_module
            super().__init__(*args, **kwargs)
            self.episode_rewards = []
            self.episode_lengths = []
            self.episode_count = 0
            self.eval_freq = eval_freq  # Evaluate every N steps
            self.eval_games = eval_games  # Games per opponent during eval
            self.last_eval_step = 0
            self._seen_ep_ids = deque()
            self._seen_ep_id_set = set()
            self.last_episode_summary = None
            self.last_opponent_probs = None
            self.intrinsic_rewards = []  # Track intrinsic reward bonuses

        def evaluate_against_all_opponents(self):
            """Run evaluation games against all opponent types and return win rates"""
            print(f"\n{'='*70}")
            print(f"RUNNING EVALUATION @ {self.num_timesteps:,} steps")
            print(f"{'='*70}")

            # Create agent wrapper from current model
            temp_model_path = os.path.join(CHECKPOINT_DIR, f"temp_eval_{self.num_timesteps}.zip")
            self.model.save(temp_model_path)
            eval_agent = RecurrentPPOAgent(file_path=temp_model_path)

            # Define all opponent types (same as training)
            opponent_types = {
                "Constant": partial(ConstantAgent),
                "Based": partial(BasedAgent),
                "Random": partial(RandomAgent),
                "Aggressive": partial(ClockworkAgent, action_sheet=AGGRESSIVE_PATTERN),
                "Defensive": partial(ClockworkAgent, action_sheet=DEFENSIVE_PATTERN),
                "Hit&Run": partial(ClockworkAgent, action_sheet=HIT_AND_RUN_PATTERN),
                "Aerial": partial(ClockworkAgent, action_sheet=AERIAL_PATTERN),
                "SpecialSpam": partial(ClockworkAgent, action_sheet=SPECIAL_SPAM_PATTERN),
            }

            results = {}
            total_wins = 0
            total_games = 0

            for opp_name, opp_factory in opponent_types.items():
                wins = 0
                for _ in range(self.eval_games):
                    match_stats = run_match(
                        agent_1=eval_agent,
                        agent_2=opp_factory,
                        max_timesteps=30*60,
                        video_path=None,
                        resolution=TRAINING_CONFIG["resolution"],
                        train_mode=False
                    )
                    if match_stats.player1_result == Result.WIN:
                        wins += 1

                win_rate = (wins / self.eval_games * 100)
                results[opp_name] = {'wins': wins, 'total': self.eval_games, 'win_rate': win_rate}
                total_wins += wins
                total_games += self.eval_games

            # Clean up temp file
            if os.path.exists(temp_model_path):
                os.remove(temp_model_path)

            # Print results table
            print(f"\n{'Opponent':<15} {'Win Rate':<12} {'Record':<10}")
            print("-" * 40)
            for opp_name, stats in results.items():
                wr = stats['win_rate']
                record = f"{stats['wins']}/{stats['total']}"
                if wr >= 70:
                    wr_str = f"{wr:>5.1f}% ‚úì"
                elif wr >= 50:
                    wr_str = f"{wr:>5.1f}%"
                else:
                    wr_str = f"{wr:>5.1f}% ‚úó"
                print(f"{opp_name:<15} {wr_str:<12} {record:<10}")

            overall_wr = (total_wins / total_games * 100) if total_games > 0 else 0
            print("-" * 40)
            print(f"{'OVERALL':<15} {overall_wr:>5.1f}%     {total_wins}/{total_games}")
            print(f"{'='*70}\n")

            self._update_curriculum(results)

            # META-LEARNING: Update self-play handler with performance data
            if hasattr(self, 'env_ref') and hasattr(self.env_ref, 'opponent_cfg'):
                for name, value in self.env_ref.opponent_cfg.opponents.items():
                    if name == "self_play" and isinstance(value[1], MetaSelfPlayHandler):
                        # For self-play opponents, we need to track which checkpoint was actually used
                        # This is approximated by the most recent checkpoint
                        import glob
                        zips = glob.glob(os.path.join(CHECKPOINT_DIR, "rl_model_*.zip"))
                        if zips:
                            # Use the most recent checkpoint as approximation
                            latest_checkpoint = max(zips, key=os.path.getctime)
                            value[1].update_performance(latest_checkpoint, overall_wr / 100.0)
                        break

            return results, overall_wr

        def _update_curriculum(self, eval_results: dict) -> None:
            """Dynamically adjust opponent sampling based on evaluation results."""
            if self.env_ref is None:
                return

            cfg = getattr(self.env_ref, "opponent_cfg", None)
            if cfg is None or not hasattr(cfg, "opponents"):
                return

            base_probs = getattr(cfg, "base_probabilities", None)
            if base_probs is None:
                base_probs = {
                    name: value[0] if isinstance(value, tuple) else value
                    for name, value in cfg.opponents.items()
                }
                cfg.base_probabilities = base_probs

            prev_probs = {
                name: value[0] if isinstance(value, tuple) else value
                for name, value in cfg.opponents.items()
            }

            new_opponents: Dict[str, Tuple[float, Any]] = {}
            for name, value in cfg.opponents.items():
                if isinstance(value, tuple):
                    current_prob, agent_factory = value
                else:
                    current_prob, agent_factory = value, None
                base_prob = base_probs.get(name, current_prob)

                adjustment = 0.0
                for eval_name, stats in eval_results.items():
                    mapped_name = EVAL_TO_TRAIN_KEY.get(eval_name)
                    if mapped_name == name:
                        win_rate = stats['win_rate'] / 100.0
                        deficit = max(0.0, CURRICULUM_CONFIG["target_win_rate"] - win_rate)
                        adjustment = deficit * CURRICULUM_CONFIG["adaptation_strength"]
                        break

                if name == "self_play":
                    adjusted_prob = max(base_prob, CURRICULUM_CONFIG["min_probability"])
                else:
                    adjusted_prob = max(CURRICULUM_CONFIG["min_probability"], base_prob + adjustment)

                new_opponents[name] = (adjusted_prob, agent_factory)

            cfg.opponents = new_opponents
            cfg.validate_probabilities()
            cfg.base_probabilities = {
                name: value[0] if isinstance(value, tuple) else value
                for name, value in cfg.opponents.items()
            }

            updated = any(
                abs(cfg.opponents[name][0] - prev_probs.get(name, 0.0)) > 1e-5
                for name in cfg.opponents
            )

            if updated:
                print("[CURRICULUM] Opponent probabilities (normalized):")
                for name, (prob, _) in cfg.opponents.items():
                    print(f"  - {name}: {prob:.3f}")
                self.last_opponent_probs = {name: prob for name, (prob, _) in cfg.opponents.items()}

            # Logger calls are disabled when tensorboard_log=None

        def _on_step(self):
            # DYNAMIC EXPLORATION SCHEDULING: Update entropy coefficient
            if self.exploration_scheduler is not None:
                new_ent_coef = self.exploration_scheduler.get_ent_coef(self.num_timesteps)
                if hasattr(self.model, 'ent_coef'):
                    self.model.ent_coef = new_ent_coef

            # Track episode completion from buffer
            if hasattr(self, 'model') and hasattr(self.model, 'ep_info_buffer'):
                buffer = list(self.model.ep_info_buffer)
                for ep_info in buffer:
                    ep_id = id(ep_info)
                    if ep_id in self._seen_ep_id_set:
                        continue
                    self._seen_ep_ids.append(ep_id)
                    self._seen_ep_id_set.add(ep_id)
                    if len(self._seen_ep_ids) > 200:
                        old_id = self._seen_ep_ids.popleft()
                        self._seen_ep_id_set.discard(old_id)

                    reward = ep_info.get('r')
                    length = ep_info.get('l', 0)
                    if reward is not None:
                        self.episode_rewards.append(reward)
                        self.episode_lengths.append(length)
                        self.episode_count += 1

                    if self.env_ref is not None and hasattr(self.env_ref, 'raw_env'):
                        summary = getattr(self.env_ref.raw_env, '_diag_last_episode', None)
                        if summary is not None:
                            self.last_episode_summary = summary
                            # Track intrinsic rewards
                            intrinsic_reward = summary.get('intrinsic_reward', 0.0)
                            if intrinsic_reward != 0.0:
                                self.intrinsic_rewards.append(intrinsic_reward)

            # Logger calls are disabled when tensorboard_log=None

            # Async curiosity updates every 2000 steps
            if self.curiosity_module is not None and self.num_timesteps % 2000 == 0:
                self.curiosity_module.update()

            # Run evaluation every eval_freq steps
            if self.num_timesteps >= self.last_eval_step + self.eval_freq and self.num_timesteps > 0:
                self.last_eval_step = self.num_timesteps
                self.evaluate_against_all_opponents()

            # Print comprehensive update every 2000 steps (reduced frequency for GPU focus)
            if self.n_calls % 2000 == 0:
                print(f"\n{'='*70}")
                print(f"TRAINING UPDATE @ {self.num_timesteps:,} steps")
                print(f"{'='*70}")

                # === PERFORMANCE METRICS ===
                if self.episode_rewards:
                    recent_rewards = self.episode_rewards[-100:]  # Last 100 episodes
                    print(f"\n[PERFORMANCE]")
                    print(f"  Episodes completed: {self.episode_count}")
                    print(f"  Avg Reward (last 100 ep): {np.mean(recent_rewards):.2f}")
                    print(f"  Reward Std: {np.std(recent_rewards):.2f}")

                if self.last_episode_summary:
                    stats = self.last_episode_summary
                    print(f"\n[EPISODE DIAGNOSTICS]")
                    print(f"  Damage dealt: {stats.get('damage_dealt', 0):.1f}")
                    print(f"  Damage taken: {stats.get('damage_taken', 0):.1f}")
                    print(f"  Time in danger zone (s): {stats.get('zone_time', 0):.1f}")
                    print(f"  Sparsity penalty: {stats.get('sparsity_penalty', 0):.2f}")
                    print(f"  Intrinsic reward: {stats.get('intrinsic_reward', 0):.3f}")
                    print(f"  Episode reward: {stats.get('reward', 0):.2f}")
                    print(f"  Episode length: {self.episode_lengths[-1] if self.episode_lengths else 0} steps")

                # Add exploration metrics
                if self.exploration_scheduler is not None:
                    current_ent_coef = self.exploration_scheduler.get_ent_coef(self.num_timesteps)
                    print(f"  Current entropy coef: {current_ent_coef:.4f}")

                if self.intrinsic_rewards:
                    recent_intrinsic = np.mean(self.intrinsic_rewards[-20:])  # Last 20 intrinsic rewards
                    print(f"  Recent intrinsic reward: {recent_intrinsic:.4f}")

                # Add curriculum info
                if hasattr(self, 'env_ref') and hasattr(self.env_ref, 'opponent_cfg'):
                    current_opponent = getattr(self.env_ref.opponent_cfg, 'current_opponent_name', 'unknown')
                    print(f"  Current opponent type: {current_opponent}")

                # === LEARNING STABILITY ===
                print(f"\n[LEARNING]")

                # GPU Memory usage (if available)
                if torch.cuda.is_available():
                    try:
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
                        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3
                        print(f"  GPU Memory: {gpu_allocated:.1f}GB used / {gpu_reserved:.1f}GB reserved / {gpu_memory:.1f}GB total")
                    except:
                        pass

                # Explained variance (how well value function predicts returns)
                if hasattr(self.model, 'rollout_buffer') and self.model.rollout_buffer.size() > 0:
                    buffer = self.model.rollout_buffer
                    if hasattr(buffer, 'returns') and hasattr(buffer, 'values'):
                        returns = buffer.returns.flatten()
                        values = buffer.values.flatten()
                        var_returns = np.var(returns)
                        if var_returns > 0:
                            explained_var = 1 - np.var(returns - values) / var_returns
                            print(f"  Explained Variance: {explained_var:.3f} {'‚úì' if explained_var > 0.5 else '‚ö†Ô∏è LOW'}")

                print(f"{'='*70}\n")

            return super()._on_step()

    training_callback = TrainingMonitor(
        env=vec_env,  # Pass vectorized environment reference for opponent tracking
        exploration_scheduler=exploration_scheduler,  # Dynamic exploration scheduling
        curiosity_module=curiosity_module,  # For async curiosity updates
        eval_freq=25_000,  # Evaluate every 25k steps (more frequent to track progress)
        eval_games=3,  # 3 games per opponent during evaluation (better statistics)
        save_freq=TRAINING_CONFIG["save_freq"],
        save_path=CHECKPOINT_DIR,
        name_prefix="rl_model",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )

    print("üöÄ HYPER-OPTIMIZED ADVANCED TRAINING STARTED\n")
    print("Version 4.2 - STATE-OF-THE-ART RecurrentPPO + MAXIMUM PERFORMANCE")
    print("="*70)
    print("  üî• GPU ACCELERATED: Curiosity models on GPU, CUDA optimizations")
    print("  ‚ö° ASYNC CURIOSITY: Batched updates every 2000 steps")
    print("  üéØ VECTORIZED ENVS: 2 parallel environments for 2x throughput")
    print("  üèÉ‚Äç‚ôÇÔ∏è AMP TRAINING: Automatic Mixed Precision for 2x speed")
    print("  ‚ú® ADVANCED LSTM: 512 hidden, 3 layers, dropout")
    print("  üß† CURIOSITY EXPLORATION: GPU-accelerated intrinsic motivation")
    print("  üìà HYPERPARAMETERS: Optimized for GPU (4096 steps, 1024 batch)")
    print("  üéØ ADVANTAGE NORMALIZATION: Stable policy updates")
    print("  üîÑ DYNAMIC ENTROPY: Exploration scheduling")
    print("  üèÜ ENHANCED CURRICULUM: Progressive opponent difficulty")
    print("  ‚ö° REDUCED CPU LOAD: Less frequent evaluation and logging")
    print("="*70 + "\n")

    # Train!
    model.learn(
        total_timesteps=TRAINING_CONFIG["total_timesteps"],
        callback=training_callback,
        log_interval=1,
    )

    # Save final model
    final_path = os.path.join(CHECKPOINT_DIR, "final_model.zip")
    model.save(final_path)

    print("\n" + "=" * 70)
    print("‚úÖ TRAINING COMPLETE!")
    print("=" * 70)
    print(f"Final model saved to: {final_path}")
    print(f"Checkpoints saved in: {CHECKPOINT_DIR}")
    print("\nTo continue training, load the checkpoint and resume!")
    print("=" * 70 + "\n")


# ============================================================================
# RUN TRAINING
# ============================================================================

if __name__ == "__main__":
    train()
