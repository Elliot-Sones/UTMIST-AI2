# ðŸŽ¯ REWARD REBALANCE FIX - Stop Reward Hacking

**Date:** 2024-10-31
**Issue:** Agent reward hacking (0% win rate despite dealing damage)
**Root Cause:** Dense rewards overwhelm sparse outcome signals

---

## ðŸš¨ **PROBLEM IDENTIFIED**

### **50k Training Results (BEFORE Fix):**
```
Step 50000:
  Win Rate: 0.0% (0/5 matches)
  Damage Ratio: 0.00
  Reward: on_attack_button_press=0.500 (every frame)

Step 40000:
  Win Rate: 0.0% (0/5 matches)
  Damage Ratio: 0.05 (agent DID deal damage!)
  But: Agent optimized button presses, not winning
```

**Agent learned WRONG lesson:**
- âœ… Learned to press attack buttons
- âœ… Learned to approach opponent
- âŒ Does NOT care about winning
- âŒ Reward hacking: spam buttons for constant +0.5, ignore win objective

---

## ðŸ“Š **REWARD ANALYSIS (Before Fix)**

**Typical 60-second episode:**

| Reward Type | Fires | Weight | Total | % |
|-------------|-------|--------|-------|---|
| `head_to_opponent` (dense) | 1500x | 10.0 | 1500 | 62% |
| `on_attack_button_press` (dense) | 150x | 15.0 | 2250 | 30% |
| `damage_interaction` (sparse) | 5x | 200 | 500 | 8% |
| **Dense Total** | - | - | **2400** | **92%** |
| **Sparse Total** | - | - | **500** | **8%** |

**Problem:** Dense rewards 4.8x larger than outcome rewards!
- Agent optimizes "approach + button spam" instead of "win fights"
- Transformer is ignored (same fixed policy works for easy targets)

---

## ðŸ”§ **THE FIX: Conservative Rebalance**

### **Changes Made (Lines 1734-1774):**

```python
# BEFORE (Reward Hacking):
'damage_interaction_reward': weight=200.0
'head_to_opponent': weight=10.0     # Dense, fires 1500x/episode
'on_attack_button_press': weight=15.0  # Dense, fires 150x/episode
'on_win_reward': weight=500

# AFTER (Outcome-Focused - CONSERVATIVE):
'damage_interaction_reward': weight=500.0   # ðŸ”¥ +150% (make damage supreme)
'head_to_opponent': weight=3.0              # ðŸ”§ -70% (safer than 1.0, maintains exploration)
'on_attack_button_press': weight=3.0        # ðŸ”§ -80% (safer than 1.0, keeps attack behavior)
'on_win_reward': weight=2000                # ðŸš€ +300% (winning is EVERYTHING)
'on_knockout_reward': weight=150            # ðŸ”¥ +200% (knockouts win fights)
'on_combo_reward': weight=50                # ðŸ”¥ +150% (combos are advanced)
```

**Why 3.0 instead of 1.0?**
- Less shock to value function (trained on old weights for 50k steps)
- Maintains stronger exploration signal (agent won't forget to attack)
- Still achieves 5:1 outcome-to-dense ratio (vs 15:1 with 1.0)
- **Confidence: 80%** (vs 60-70% with 1.0 weights)

---

## ðŸ“Š **NEW REWARD BALANCE (After Fix)**

**Typical 60-second episode (with 3.0 weights):**

| Reward Type | Fires | Weight | Total | % |
|-------------|-------|--------|-------|---|
| `head_to_opponent` (dense) | 1500x | 3.0 | 450 | 8% |
| `on_attack_button_press` (dense) | 150x | 3.0 | 450 | 8% |
| `damage_interaction` (sparse) | 5x | 500 | 2500 | 44% |
| `on_win_reward` (sparse) | 1x | 2000 | 2000 | 35% |
| `on_knockout_reward` (sparse) | 1x | 150 | 150 | 3% |
| **Dense Total** | - | - | **900** | **16%** |
| **Sparse Total** | - | - | **4650** | **84%** |

**Now:** Outcome rewards 5.2x larger than dense guidance!
- Agent MUST optimize for wins/damage, not button spam
- Dense rewards provide minimal guidance (not dominant signal)
- **Transformer becomes critical:** Must analyze opponent to win

---

## âœ… **WHY THIS FIXES THE PROBLEM**

### **1. Agent Already Knows Fundamentals**
Evidence from step 40k:
- âœ… Agent deals damage (ratio 0.05 > 0)
- âœ… Agent presses attack buttons
- âœ… Agent approaches opponent

**Conclusion:** Training wheels no longer needed!

### **2. Dense Rewards Now "Guardrails" Not "Goals"**
- `head_to_opponent=3.0`: Moderate guidance to engage (reduced from 10.0)
- `on_attack_button_press=3.0`: Encourages attacks (reduced from 15.0)
- Still provides exploration signal but not dominant reward source

### **3. Winning Becomes Dominant Objective**
```python
# Episode reward breakdown (new balance with 3.0 weights):
Button spam strategy:  +900 (dense only, no wins)
Winning strategy:      +4650 (damage + win + knockout)

Ratio: Winning is 5.2x more rewarding than hacking!
```

### **4. Transformer Must Matter Now**
- ConstantAgent is stationary â†’ Easy to beat with basic attacks
- But to get +2000 win reward, agent must actually WIN
- Can't just spam buttons and farm approach rewards
- **Must learn effective strategies â†’ Transformer encoding becomes useful**

---

## ðŸŽ¯ **EXPECTED RESULTS (Next 50k Run)**

### **Early Phase (0-10k):**
- Agent continues attacking (learned behavior from previous run)
- Win rate starts climbing: 0% â†’ 20%
- Damage ratio increases: 0.05 â†’ 0.3

### **Middle Phase (10k-30k):**
- Agent discovers winning gives MASSIVE reward (+2000)
- Starts optimizing for knockouts (+150) not just approach
- Win rate: 20% â†’ 60% vs ConstantAgent

### **Late Phase (30k-50k):**
- Refined tactics: combos, weapon usage, knockout sequences
- Win rate: 60% â†’ 85%+ vs ConstantAgent
- **Ready for Stage 2:** BasedAgent (moving opponent)

---

## ðŸ“ˆ **MONITORING SUCCESS**

Watch for these milestones in next training run:

**âœ… Good Signs:**
```
--- Step 15000 ---
  Reward Breakdown: damage_interaction_reward=2.143, on_win_reward=1.000
  Win Rate: 40.0% (2/5 matches)
  Damage Ratio: 1.25
```

**ðŸš¨ Still Failing:**
```
--- Step 30000 ---
  Reward Breakdown: on_attack_button_press=0.500, head_to_opponent=0.100
  Win Rate: 0.0% (0/5 matches)
  ðŸ‘† If this happens, rewards STILL too dense - reduce further
```

---

## ðŸ”„ **NEXT STEPS IF SUCCESSFUL**

If agent achieves **80%+ win rate vs ConstantAgent** by 50k:

**Stage 2 (Next 50k):** Switch opponent to BasedAgent
- Change line 479: `"based_agent": (1.0, partial(BasedAgent))`
- **Keep same reward weights** (they're now outcome-focused)
- Agent must learn to beat MOVING opponent

**Stage 3 (200k-1M):** Introduce self-play
- 70% self-play, 30% scripted opponents
- **Reduce dense rewards further:** `head_to_opponent=0.5`, `attack_button=0.5`
- Transformer becomes essential (diverse opponents)

**Stage 4 (1M-10M):** Competition training
- **Remove dense rewards entirely:** Set both to 0.0
- 100% sparse rewards (damage, win, knockout, combo)
- Agent relies ONLY on transformer + outcome optimization

---

## ðŸš¨ **ROLLBACK (If Needed)**

**Current implementation already uses conservative 3.0 weights** (safer approach).

If agent still breaks (stops attacking entirely):

```python
# Revert to even more moderate balance:
'head_to_opponent': weight=5.0  # Was 3.0, was 10.0 originally
'on_attack_button_press': weight=5.0  # Was 3.0, was 15.0 originally
'damage_interaction_reward': weight=400.0  # Keep high
'on_win_reward': weight=1500  # Reduce slightly
```

If agent succeeds and you want more aggressive rebalance later:

```python
# Future Stage 3 (after 100k successful steps):
'head_to_opponent': weight=1.0  # Further reduction
'on_attack_button_press': weight=1.0  # Further reduction
# Keep outcome rewards high (500, 2000)
```

---

## ðŸ“ž **JUSTIFICATION**

**Why this is safe:**
1. Agent already proved it CAN attack (damage > 0 at step 40k)
2. We're not removing rewards, just rebalancing ratios
3. Dense rewards still present (1.0 weight) as gentle guidance
4. Massive increase in outcome rewards gives clear objective

**Why this is necessary:**
1. 0% win rate after 50k steps is catastrophic failure
2. Agent reward hacking proves reward structure is broken
3. Transformer is worthless if fixed policy always works
4. Competition requires adaptability, not button spam

**Why this will work:**
1. Dense 6% vs Sparse 94% = outcomes 15.5x more valuable
2. Agent can't hack its way to max reward anymore
3. Only path to high reward: win fights effectively
4. Transformer analysis becomes critical for diverse opponents

---

## âœ… **FILES MODIFIED**

- [user/train_agent.py](user/train_agent.py) - Lines 1734-1801 (reward weights)

---

**Ready to run! ðŸš€ This should fix the reward hacking and teach the agent to WIN, not just spam buttons.**
